{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4Py Type4Py Top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benji/Documents/Uni/heidelberg/05/masterarbeit/impls/scripts/experiments\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pwd\n",
    "%cd /home/benji/Documents/Uni/heidelberg/05/masterarbeit/impls/scripts/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(300)\n",
    "pl.Config.set_tbl_rows(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from scripts.common.schemas import TypeCollectionCategory\n",
    "from scripts.infer.structure import DatasetFolderStructure\n",
    "\n",
    "tool = \"type4pyN1\"\n",
    "dataset = DatasetFolderStructure(pathlib.Path(\n",
    "    \"/home/benji/Documents/Uni/heidelberg/05/masterarbeit/datasets/better-types-4-py-dataset\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[type4pyN1 @ INFO]: Hello World!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "\n",
    "logging.shutdown()\n",
    "reload(logging)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "ch.setFormatter(logging.Formatter(f\"[{tool} @ %(levelname)s]: %(message)s\"))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(\"Hello World!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction Metrics for Full Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 11:00:57.436176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 11:00:59.501579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110981c6f7db4a32a85db4e6a194b94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "# Common Type Names\n",
    "from typet5.model import ModelWrapper\n",
    "model = ModelWrapper.load_from_hub(\"MrVPlusOne/TypeT5-v7\")\n",
    "common_names = model.common_type_names\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.common.schemas import TypeCollectionCategory\n",
    "\n",
    "import pprint\n",
    "\n",
    "# Because our analysis reviews more datapoints than these models actually regard, reuse TypeT5 metrics instead\n",
    "def typet5_metrics_4_type4py(task: TypeCollectionCategory | str) -> None:\n",
    "    from scripts.common.output import InferenceArtifactIO\n",
    "\n",
    "    test_set = dataset.test_set()\n",
    "\n",
    "    proj2datasets = [\n",
    "        (\n",
    "            project,\n",
    "            InferenceArtifactIO(\n",
    "                artifact_root=pathlib.Path(),\n",
    "                dataset=dataset,\n",
    "                repository=project,\n",
    "                tool_name=tool,\n",
    "                task=task,\n",
    "            ),\n",
    "        )\n",
    "        for project in test_set\n",
    "    ]\n",
    "    existing = dict(\n",
    "        (project, artifact)\n",
    "        for project, artifact in proj2datasets\n",
    "        if artifact.full_location().exists()\n",
    "    )\n",
    "    \n",
    "    from importlib import reload\n",
    "\n",
    "    from libcst import helpers as h\n",
    "    import tqdm\n",
    "\n",
    "\n",
    "    from typet5.experiments import type4py\n",
    "    from typet5.static_analysis import PythonProject, SignatureMap, AccuracyMetric, SignatureErrorAnalysis\n",
    "    from typet5.experiments.typet5 import accs_as_table_row\n",
    "    from typet5.visualization import pretty_print_dict\n",
    "    reload(type4py)\n",
    "\n",
    "    assignments = []\n",
    "    projects = []\n",
    "\n",
    "    for ctr, (project, artifact) in tqdm.tqdm(enumerate(existing.items()), desc=f\"Loading labels and predictions from {task}\"):            \n",
    "        type4py_predictions, = artifact.read()\n",
    "        # if ctr < 1:\n",
    "            # pprint.pprint(type4py_predictions)\n",
    "\n",
    "        for file, predictions in type4py_predictions.items():\n",
    "            modpkg = h.calculate_module_and_package(repo_root=project, filename=project / file)\n",
    "            parser = type4py.Type4PyResponseParser(modpkg.name)\n",
    "            assignments.append(parser.parse({\"response\": predictions}))\n",
    "\n",
    "        projects.append(PythonProject.parse_from_root(project))\n",
    "\n",
    "\n",
    "    name2project = {p.name: p for p in projects}\n",
    "    \n",
    "    label_signatures: dict[str, SignatureMap] = {\n",
    "        project.name: {e.path: e.get_signature() for e in project.all_elems()}\n",
    "        for project in projects\n",
    "    }\n",
    "    pred_signatures: dict[str, SignatureMap] = {n: dict() for n in label_signatures}\n",
    "\n",
    "    module_srcs = [\n",
    "        (project.name, name)\n",
    "        for project in projects\n",
    "        for name in project.modules\n",
    "    ]\n",
    "    for (pname, mname), o in zip(module_srcs, assignments):\n",
    "        if isinstance(o, str):\n",
    "            if list(name2project[pname].modules[mname].all_elements()):\n",
    "                # only warn for non-empty modules\n",
    "                logger.warning(f\"In project {pname} module {mname}, Type4Py errored: {o}\")\n",
    "        else:\n",
    "            pred_signatures[pname].update(o)\n",
    "\n",
    "    # print(pred_signatures)\n",
    "\n",
    "    eval_result = type4py.Type4PyEvalResult(\n",
    "        pred_maps=pred_signatures,\n",
    "        label_maps=label_signatures,\n",
    "    )\n",
    "    \n",
    "    metrics = AccuracyMetric.default_metrics(common_type_names=common_names)\n",
    "    # acc_metric = AccuracyMetric(common_type_names=ubiq_names)\n",
    "\n",
    "    n_annots = sum([e.get_signature().n_annots() for p in projects for e in p.all_elems()])\n",
    "    n_labels = sum([e.n_annotated() for lm in eval_result.label_maps.values() for e in lm.values()])\n",
    "    \n",
    "    logger.info(f\"n_annots: {n_annots}, n_labels: {n_labels}\")\n",
    "    logger.info(f\"Ratio: {n_labels / n_annots}\")\n",
    "    \n",
    "    accs = {\n",
    "        m.name: SignatureErrorAnalysis(\n",
    "            eval_result.pred_maps,\n",
    "            eval_result.label_maps,\n",
    "            m,\n",
    "            error_on_mismatched_signature=False,\n",
    "        ).accuracies\n",
    "        for m in metrics\n",
    "    }\n",
    "    accs_as_table_row(accs)\n",
    "    # pretty_print_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels and predictions from VARIABLE: 50it [04:36,  5.54s/it]\n",
      "[type4pyN1 @ INFO]: n_annots: 30070, n_labels: 16520\n",
      "[type4pyN1 @ INFO]: Ratio: 0.5493847688726305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "42.48 & 46.62 & 48.49 & 29.27 & 48.79\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "60.78 & 63.01 & 64.80 & 45.48 & 61.35\n",
      "Accuracies on rare types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "1.20 & 19.04 & 20.67 & 5.13 & 22.09\n"
     ]
    }
   ],
   "source": [
    "typet5_metrics_4_type4py(task=TypeCollectionCategory.VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels and predictions from CALLABLE_RETURN: 50it [04:47,  5.75s/it]\n",
      "[type4pyN1 @ INFO]: n_annots: 30070, n_labels: 16520\n",
      "[type4pyN1 @ INFO]: Ratio: 0.5493847688726305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "42.48 & 46.69 & 48.59 & 29.26 & 48.87\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "60.79 & 63.14 & 64.95 & 45.62 & 61.49\n",
      "Accuracies on rare types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "1.20 & 19.03 & 20.69 & 5.05 & 22.08\n"
     ]
    }
   ],
   "source": [
    "typet5_metrics_4_type4py(task=TypeCollectionCategory.CALLABLE_RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels and predictions from CALLABLE_PARAMETER: 50it [05:06,  6.13s/it]\n",
      "[type4pyN1 @ INFO]: n_annots: 30070, n_labels: 16520\n",
      "[type4pyN1 @ INFO]: Ratio: 0.5493847688726305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "42.48 & 46.69 & 48.59 & 29.26 & 48.87\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "60.79 & 63.14 & 64.95 & 45.62 & 61.49\n",
      "Accuracies on rare types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "1.20 & 19.03 & 20.69 & 5.05 & 22.08\n"
     ]
    }
   ],
   "source": [
    "typet5_metrics_4_type4py(task=TypeCollectionCategory.CALLABLE_PARAMETER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels and predictions from all: 50it [03:41,  4.44s/it]\n",
      "[type4pyN1 @ INFO]: n_annots: 30070, n_labels: 16520\n",
      "[type4pyN1 @ INFO]: Ratio: 0.5493847688726305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "42.48 & 46.62 & 48.49 & 29.27 & 48.79\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "60.78 & 63.01 & 64.80 & 45.48 & 61.35\n",
      "Accuracies on rare types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "1.20 & 19.04 & 20.67 & 5.13 & 22.09\n"
     ]
    }
   ],
   "source": [
    "typet5_metrics_4_type4py(task=\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb5ea0ec9276ea53bca768cbad83872d3e07fe0f98ee4aa691b5a250409cc09e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
