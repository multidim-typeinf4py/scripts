{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4Py TT5 Top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nfs/home/bsparks/mdti4py/scripts/scripts']\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "print(scripts.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetterTypes4Py @ /nfs/home/bsparks/mdti4py/datasets/better-types-4-py-dataset\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "from scripts.common.schemas import TypeCollectionCategory\n",
    "from scripts.infer.structure import DatasetFolderStructure\n",
    "\n",
    "tool = \"TypeT5TopN1\"\n",
    "dataset = DatasetFolderStructure(pathlib.Path(\n",
    "    \"/nfs/home/bsparks/mdti4py/datasets/better-types-4-py-dataset\"\n",
    "))\n",
    "assert dataset.dataset_root.is_dir(), f\"{dataset.dataset_root} not a directory!\"\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TypeT5TopN1 @ INFO]: Hello World!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "\n",
    "logging.shutdown()\n",
    "reload(logging)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "ch.setFormatter(logging.Formatter(f\"[{tool} @ %(levelname)s]: %(message)s\"))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction Metrics for Full Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colored in /nfs/home/bsparks/.conda/envs/poetry/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: plotly in /nfs/home/bsparks/.conda/envs/poetry/lib/python3.10/site-packages (5.16.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /nfs/home/bsparks/.conda/envs/poetry/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /nfs/home/bsparks/.conda/envs/poetry/lib/python3.10/site-packages (from plotly) (23.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install colored plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ground truths: 100%|█| 50/50 [01:18<00:00,  1.57s/it, project=/nfs/home/bsparks/mdti4py/datasets/better-types-\n"
     ]
    }
   ],
   "source": [
    "# Because our analysis reviews more datapoints than these models actually regard, reuse TypeT5 metrics instead\n",
    "import tqdm\n",
    "\n",
    "from typet5.static_analysis import PythonProject, SignatureMap, AccuracyMetric, SignatureErrorAnalysis\n",
    "from typet5.experiments.typet5 import accs_as_table_row\n",
    "from typet5.visualization import pretty_print_dict\n",
    "\n",
    "from scripts.common.output import InferenceArtifactIO\n",
    "\n",
    "test_set = dataset.test_set()\n",
    "\n",
    "projects = dict()\n",
    "for project in (pbar := tqdm.tqdm(test_set, desc=f\"Loading ground truths\")):\n",
    "    pbar.set_postfix({\"project\": str(project)})\n",
    "    projects[project.name] = PythonProject.parse_from_root(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824af2871ec4514abcd7479b80f06a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "# Common Type Names\n",
    "from typet5.model import ModelWrapper\n",
    "model = ModelWrapper.load_from_hub(\"MrVPlusOne/TypeT5-v7\")\n",
    "common_names = model.common_type_names\n",
    "del model\n",
    "\n",
    "from scripts.common.schemas import TypeCollectionCategory\n",
    "\n",
    "# Because our analysis reviews more datapoints than these models actually regard, reuse TypeT5 metrics instead\n",
    "from scripts.common.output import InferenceArtifactIO\n",
    "\n",
    "\n",
    "def type_t5_metrics(task: TypeCollectionCategory) -> None:\n",
    "    assignments = dict()\n",
    "\n",
    "    for project in (pbar := tqdm.tqdm(test_set, desc=f\"Loading predictions for {task}\")):\n",
    "        artifact = InferenceArtifactIO(\n",
    "            artifact_root=pathlib.Path(\"/nfs/home/bsparks/mdti4py/datasets\"),\n",
    "            dataset=dataset,\n",
    "            repository=project,\n",
    "            tool_name=tool,\n",
    "            task=task\n",
    "        )\n",
    "        pbar.set_postfix({\"project\": artifact.relative_location()})\n",
    "        \n",
    "        (tt5_predictions, tt5logits) = artifact.read()\n",
    "        #assignments[project.name] = {key: pred[0] for key, pred in tt5_predictions.items()}\n",
    "        assignments[project.name] = tt5_predictions\n",
    "\n",
    "                \n",
    "    label_signatures: dict[str, SignatureMap] = {\n",
    "        project_name: {e.path: e.get_signature() for e in labels.all_elems()}\n",
    "        for project_name, labels in projects.items()\n",
    "    }\n",
    "    # pred_signatures: dict[str, SignatureMap] = {n: dict() for n in label_signatures}\n",
    "        \n",
    "    pred_signatures = assignments \n",
    "    \n",
    "    metrics = AccuracyMetric.default_metrics(common_type_names=common_names)\n",
    "    # acc_metric = AccuracyMetric(common_type_names=ubiq_names)\n",
    "\n",
    "    n_annots = sum([e.get_signature().n_annots() for _, p in projects.items() for e in p.all_elems()])\n",
    "    n_labels = sum([e.n_annotated() for lm in label_signatures.values() for e in lm.values()])\n",
    "\n",
    "    logger.info(f\"n_annots: {n_annots}, n_labels: {n_labels}\")\n",
    "    logger.info(f\"Ratio: {n_labels / n_annots}\")\n",
    "\n",
    "    accs = {\n",
    "        m.name: SignatureErrorAnalysis(\n",
    "            pred_signatures,\n",
    "            label_signatures,\n",
    "            m,\n",
    "            error_on_mismatched_signature=False,\n",
    "        ).accuracies\n",
    "        for m in metrics\n",
    "    }\n",
    "    accs_as_table_row(accs)\n",
    "    pretty_print_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading predictions for all: 100%|█| 50/50 [00:03<00:00, 16.41it/s, project=BetterTypes4Py/kornicameister__axion/TypeT\n",
      "[TypeT5TopN1 @ INFO]: n_annots: 30070, n_labels: 16520\n",
      "[TypeT5TopN1 @ INFO]: Ratio: 0.5493847688726305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "66.68 & 67.83 & 75.48 & 35.89 & 74.14\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "76.39 & 78.88 & 84.61 & 48.72 & 83.19\n",
      "Accuracies on rare types:\n",
      "header:  ['full.all', 'calibrated.all', 'calibrated.simple', 'calibrated.complex', 'base.all']\n",
      "49.05 & 52.62 & 61.60 & 24.13 & 58.24\n",
      "full_acc:\n",
      "   full_acc: 66.68% (count=16.5k)\n",
      "   full_acc_by_cat:\n",
      "      FuncArg: 61.26% (count=8.4k)\n",
      "      FuncReturn: 77.50% (count=6.0k)\n",
      "      ClassAtribute: 57.23% (count=2.0k)\n",
      "      GlobalVar: 60.18% (count=113)\n",
      "   full_acc_by_simple:\n",
      "      complex: 34.92% (count=3.4k)\n",
      "      simple: 75.00% (count=13.1k)\n",
      "   full_acc_label_size: 1.4214\n",
      "   full_acc_pred_size: 1.4212\n",
      "   full_acc_ignored_labels: 0\n",
      "full_acc_common:\n",
      "   full_acc_common: 76.39% (count=10.7k)\n",
      "   full_acc_common_by_cat:\n",
      "      FuncArg: 76.54% (count=5.1k)\n",
      "      FuncReturn: 76.40% (count=4.3k)\n",
      "      ClassAtribute: 75.84% (count=1.2k)\n",
      "      GlobalVar: 74.32% (count=74)\n",
      "   full_acc_common_by_simple:\n",
      "      complex: 43.63% (count=1.8k)\n",
      "      simple: 82.91% (count=8.9k)\n",
      "   full_acc_common_label_size: 1.3744\n",
      "   full_acc_common_pred_size: 1.3131\n",
      "   full_acc_common_ignored_labels: 5867\n",
      "full_acc_rare:\n",
      "   full_acc_rare: 49.05% (count=5.9k)\n",
      "   full_acc_rare_by_cat:\n",
      "      FuncArg: 49.41% (count=2.8k)\n",
      "      FuncReturn: 49.05% (count=2.4k)\n",
      "      ClassAtribute: 47.33% (count=600)\n",
      "      GlobalVar: 50.00% (count=48)\n",
      "   full_acc_rare_by_simple:\n",
      "      complex: 25.65% (count=1.7k)\n",
      "      simple: 58.30% (count=4.2k)\n",
      "   full_acc_rare_label_size: 1.5067\n",
      "   full_acc_rare_pred_size: 1.6175\n",
      "   full_acc_rare_ignored_labels: 10653\n",
      "acc:\n",
      "   acc: 67.83% (count=13.9k)\n",
      "   acc_by_cat:\n",
      "      FuncArg: 66.87% (count=7.0k)\n",
      "      FuncReturn: 68.76% (count=5.1k)\n",
      "      ClassAtribute: 69.27% (count=1.7k)\n",
      "      GlobalVar: 63.53% (count=85)\n",
      "   acc_by_simple:\n",
      "      complex: 35.89% (count=2.7k)\n",
      "      simple: 75.48% (count=11.2k)\n",
      "   acc_label_size: 1.3099\n",
      "   acc_pred_size: 1.3751\n",
      "   acc_ignored_labels: 2621\n",
      "acc_common:\n",
      "   acc_common: 78.88% (count=8.1k)\n",
      "   acc_common_by_cat:\n",
      "      FuncArg: 79.05% (count=3.8k)\n",
      "      FuncReturn: 79.00% (count=3.4k)\n",
      "      ClassAtribute: 77.92% (count=847)\n",
      "      GlobalVar: 75.41% (count=61)\n",
      "   acc_common_by_simple:\n",
      "      complex: 48.72% (count=1.3k)\n",
      "      simple: 84.61% (count=6.8k)\n",
      "   acc_common_label_size: 1.2924\n",
      "   acc_common_pred_size: 1.2775\n",
      "   acc_common_ignored_labels: 8470\n",
      "acc_rare:\n",
      "   acc_rare: 52.62% (count=5.8k)\n",
      "   acc_rare_by_cat:\n",
      "      FuncArg: 53.09% (count=2.8k)\n",
      "      FuncReturn: 53.13% (count=2.4k)\n",
      "      ClassAtribute: 47.67% (count=600)\n",
      "      GlobalVar: 62.50% (count=48)\n",
      "   acc_rare_by_simple:\n",
      "      complex: 24.13% (count=1.4k)\n",
      "      simple: 61.60% (count=4.4k)\n",
      "   acc_rare_label_size: 1.3339\n",
      "   acc_rare_pred_size: 1.5093\n",
      "   acc_rare_ignored_labels: 10671\n",
      "base_acc:\n",
      "   base_acc: 74.14% (count=13.9k)\n",
      "   base_acc_by_cat:\n",
      "      FuncArg: 73.04% (count=7.0k)\n",
      "      FuncReturn: 75.08% (count=5.1k)\n",
      "      ClassAtribute: 76.12% (count=1.7k)\n",
      "      GlobalVar: 69.41% (count=85)\n",
      "   base_acc_ignored_labels: 2621\n",
      "base_acc_common:\n",
      "   base_acc_common: 83.19% (count=8.9k)\n",
      "   base_acc_common_by_cat:\n",
      "      FuncArg: 82.22% (count=4.1k)\n",
      "      FuncReturn: 84.38% (count=3.6k)\n",
      "      ClassAtribute: 82.36% (count=1.2k)\n",
      "      GlobalVar: 93.75% (count=64)\n",
      "   base_acc_common_ignored_labels: 7662\n",
      "base_acc_rare:\n",
      "   base_acc_rare: 58.24% (count=5.0k)\n",
      "   base_acc_rare_by_cat:\n",
      "      FuncArg: 60.93% (count=2.4k)\n",
      "      FuncReturn: 58.59% (count=2.1k)\n",
      "      ClassAtribute: 44.25% (count=513)\n",
      "      GlobalVar: 59.38% (count=32)\n",
      "   base_acc_rare_ignored_labels: 11479\n"
     ]
    }
   ],
   "source": [
    "type_t5_metrics(task=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading predictions for CALLABLE_PARAMETER:   0%| | 0/50 [00:00<?, ?it/s, project=BetterTypes4Py/nubark__instark/TypeT\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/home/bsparks/mdti4py/datasets/BetterTypes4Py/nubark__instark/TypeT5TopN1/CALLABLE_PARAMETER/TypeT5TopN1-artifacts.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtype_t5_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTypeCollectionCategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCALLABLE_PARAMETER\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m, in \u001b[0;36mtype_t5_metrics\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     18\u001b[0m artifact \u001b[38;5;241m=\u001b[39m InferenceArtifactIO(\n\u001b[1;32m     19\u001b[0m     artifact_root\u001b[38;5;241m=\u001b[39mpathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nfs/home/bsparks/mdti4py/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m: artifact\u001b[38;5;241m.\u001b[39mrelative_location()})\n\u001b[0;32m---> 27\u001b[0m (tt5_predictions, tt5logits) \u001b[38;5;241m=\u001b[39m \u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#assignments[project.name] = {key: pred[0] for key, pred in tt5_predictions.items()}\u001b[39;00m\n\u001b[1;32m     29\u001b[0m assignments[project\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m tt5_predictions\n",
      "File \u001b[0;32m~/mdti4py/scripts/scripts/common/output.py:32\u001b[0m, in \u001b[0;36mArtifactIO.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mdti4py/scripts/scripts/common/output.py:226\u001b[0m, in \u001b[0;36mInferenceArtifactIO._read\u001b[0;34m(self, input_location)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_location: pathlib\u001b[38;5;241m.\u001b[39mPath) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[typing\u001b[38;5;241m.\u001b[39mAny]:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43minput_location\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/.conda/envs/poetry/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/home/bsparks/mdti4py/datasets/BetterTypes4Py/nubark__instark/TypeT5TopN1/CALLABLE_PARAMETER/TypeT5TopN1-artifacts.pickle'"
     ]
    }
   ],
   "source": [
    "type_t5_metrics(task=TypeCollectionCategory.CALLABLE_PARAMETER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_t5_metrics(task=TypeCollectionCategory.CALLABLE_RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type_t5_metrics(task=TypeCollectionCategory.VARIABLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb5ea0ec9276ea53bca768cbad83872d3e07fe0f98ee4aa691b5a250409cc09e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
