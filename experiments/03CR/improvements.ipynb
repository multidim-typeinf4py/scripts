{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c38bed69-e96c-48b0-aa2f-40a4c2ef417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b256049-96d6-46c9-8688-915f500babc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b113ed2-801e-415d-bbfe-8e16f0c4adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "06033218-7153-4247-9b91-a88450bf0c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nfs/home/bsparks/mdti4py/scripts/scripts']\n",
      "BetterTypes4Py @ /nfs/home/bsparks/mdti4py/datasets/better-types-4-py-dataset\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "print(scripts.__path__)\n",
    "import pathlib\n",
    "\n",
    "from scripts.common.schemas import TypeCollectionCategory\n",
    "from scripts.infer.structure import DatasetFolderStructure\n",
    "\n",
    "dataset = DatasetFolderStructure(pathlib.Path(\n",
    "    \"/nfs/home/bsparks/mdti4py/datasets/better-types-4-py-dataset\"\n",
    "))\n",
    "assert dataset.dataset_root.is_dir(), f\"{dataset.dataset_root} not a directory!\"\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dd39e07a-c02e-48ed-9734-8cd80984b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import experiments.inferred\n",
    "import experiments.probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "63390e27-6743-4137-aea5-f3379d10c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_root = pathlib.Path(\"/nfs/home/bsparks/mdti4py/datasets\")\n",
    "assert artifact_root.is_dir(), f\"Cannot find {artifact_root=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740c489-924d-4206-841b-6454899db94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(experiments.inferred)\n",
    "groundtruth = experiments.inferred.load_groundtruths(artifact_root, dataset)\n",
    "\n",
    "trivial_mask = groundtruth.base_anno.isin([\"None\", \"Any\"])\n",
    "groundtruth = groundtruth[~trivial_mask]\n",
    "\n",
    "display(groundtruth.shape, groundtruth.columns)\n",
    "# display(groundtruth.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305350d-0b94-4761-9ff5-264ec62ba27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type4py = experiments.inferred.load_entire_inferred(artifact_root, dataset, tool_name=\"type4pyN1\", task=\"all\")\n",
    "type4py_probas = experiments.probas.load_inferred_with_probablities(artifact_root, dataset, tool_name=\"type4py\", task=\"all\", inferred=type4py)\n",
    "\n",
    "anno_no_prob = type4py_probas[\"probability\"].isna() & type4py_probas[\"anno\"].notna()\n",
    "type4py_probas.loc[anno_no_prob, \"anno\"] = pd.NA\n",
    "\n",
    "# print((type4py_probas[\"probability\"].isna() & type4py_probas[\"anno\"].notna()).any())#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa207140-c02a-4869-8bfd-067ddf4eec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "typilus = experiments.inferred.load_entire_inferred(artifact_root, dataset, tool_name=\"typilusN1\", task=\"all\")\n",
    "typilus_probas = experiments.probas.load_inferred_with_probablities(artifact_root, dataset, tool_name=\"typilus\", task=\"all\", inferred=typilus)\n",
    "\n",
    "anno_no_prob = typilus_probas[\"probability\"].isna() & typilus_probas[\"anno\"].notna()\n",
    "typilus_probas.loc[anno_no_prob, \"anno\"] = pd.NA\n",
    "\n",
    "#print((typilus_probas[\"probability\"].isna() & typilus_probas[\"anno\"].notna()).any())#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c32208-1370-4bd4-ad6e-bc2e4c68f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "typet5 = experiments.inferred.load_entire_inferred(artifact_root, dataset, tool_name=\"TypeT5TopN1\", task=\"all\")\n",
    "typet5_probas = experiments.probas.load_inferred_with_probablities(artifact_root, dataset, tool_name=\"typet5\", task=\"all\", inferred=typet5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545a011-a5ab-4ffb-808b-761923dcd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = [\"topn\"]\n",
    "prediction_annos = [\"anno_type4py\", \"anno_typilus\", \"anno_typet5\"]\n",
    "methods = [\"method_type4py\", \"method_typilus\", \"method_typet5\"]\n",
    "probs = [\"probability_type4py\", \"probability_typilus\", \"probability_typet5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bbeda-9cdb-4f91-83aa-61875c7b7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "type4py_adjusted = experiments.inferred.typet5_adjusted_form(type4py_probas)\n",
    "typilus_adjusted = experiments.inferred.typet5_adjusted_form(typilus_probas)\n",
    "typet5_adjusted = experiments.inferred.typet5_adjusted_form(typet5_probas)\n",
    "\n",
    "aligned_adjusted = type4py_adjusted.drop(columns=ignore).merge(\n",
    "    typilus_adjusted.drop(columns=ignore), \n",
    "    how=\"outer\", \n",
    "    on=[\"repository\", \"category\", \"file\", \"qname\", \"qname_ssa\"], \n",
    "    suffixes=(\"_type4py\", \"_typilus\")\n",
    ").merge(\n",
    "    typet5_adjusted.drop(columns=ignore).rename(columns={\"anno\": \"anno_typet5\", \"probability\": \"probability_typet5\", \"method\": \"method_typet5\"}),\n",
    "    how=\"outer\", \n",
    "    on=[\"repository\", \"category\", \"file\", \"qname\", \"qname_ssa\"]\n",
    ")\n",
    "\n",
    "aligned_adjusted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fb6c3-fc06-4be5-adbe-b7f60cd4095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type4py_base = experiments.inferred.typet5_base_form(type4py_probas)\n",
    "typilus_base = experiments.inferred.typet5_base_form(typilus_probas)\n",
    "typet5_base = experiments.inferred.typet5_base_form(typet5_probas)\n",
    "\n",
    "aligned_base = type4py_base.drop(columns=ignore).merge(\n",
    "    typilus_base.drop(columns=ignore), \n",
    "    how=\"outer\", \n",
    "    on=[\"repository\", \"category\", \"file\", \"qname\", \"qname_ssa\"], \n",
    "    suffixes=(\"_type4py\", \"_typilus\")\n",
    ").merge(\n",
    "    typet5_base.drop(columns=ignore).rename(columns={\"anno\": \"anno_typet5\", \"probability\": \"probability_typet5\", \"method\": \"method_typet5\"}),\n",
    "    how=\"outer\", \n",
    "    on=[\"repository\", \"category\", \"file\", \"qname\", \"qname_ssa\"]\n",
    ")\n",
    "\n",
    "aligned_base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64ae52-1890-43a9-ae4c-f00b41eb0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.common.schemas import ExtendedTypeCollectionSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc44f2e-3e24-42b1-a6a3-99842b5c8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(experiments.inferred)\n",
    "joined_adjusted = experiments.inferred.join_truth_to_preds(\n",
    "    truth=groundtruth,\n",
    "    predictions=aligned_adjusted,\n",
    "    comparable_anno=ExtendedTypeCollectionSchema.adjusted_anno,\n",
    "    prediction_annos=prediction_annos,\n",
    ")\n",
    "\n",
    "adjusted_eval = experiments.inferred.evaluatable(joined_adjusted, clean_annos=prediction_annos).replace(\"<MISSING>\", pd.NA)\n",
    "\n",
    "print(adjusted_eval.info())\n",
    "adjusted_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09d2bb-d8e3-4575-9417-5af6d685301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_base = experiments.inferred.join_truth_to_preds(\n",
    "    truth=groundtruth,\n",
    "    predictions=aligned_base,\n",
    "    comparable_anno=ExtendedTypeCollectionSchema.base_anno,\n",
    "    prediction_annos=[\"anno_type4py\", \"anno_typilus\", \"anno_typet5\"]\n",
    ")\n",
    "joined_base \n",
    "\n",
    "base_eval = experiments.inferred.evaluatable(joined_base, clean_annos=prediction_annos).replace(\"<MISSING>\", pd.NA)\n",
    "\n",
    "print(base_eval.info())\n",
    "display(base_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc3a5a-1cc6-4b15-9805-5d28b16cee01",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f6ecc-7571-45de-b548-db00ce8f193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adjusted_predictions := adjusted_eval[prediction_annos])\n",
    "display(adjusted_probabilities := adjusted_eval[probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ed49d-7366-4804-ab78-8943153f6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.predictions\n",
    "\n",
    "#print(\"Adjusted\")\n",
    "typilus_adjusted_accuracy = experiments.predictions.performance(\n",
    "    adjusted_eval.rename(columns={\"anno_typilus\": \"anno\"}),\n",
    "    total=True\n",
    ")\n",
    "type4py_adjusted_accuracy = experiments.predictions.performance(\n",
    "    adjusted_eval.rename(columns={\"anno_type4py\": \"anno\"}),\n",
    "    total=True\n",
    ")\n",
    "typet5_adjusted_accuracy = experiments.predictions.performance(\n",
    "    adjusted_eval.rename(columns={\"anno_typet5\": \"anno\"}),\n",
    "    total=True\n",
    ")\n",
    "\n",
    "#print(\"Base\")\n",
    "typilus_base_accuracy = experiments.predictions.performance(\n",
    "    base_eval.rename(columns={\"anno_typilus\": \"anno\"}),\n",
    "    total=True\n",
    ")\n",
    "type4py_base_accuracy = experiments.predictions.performance(\n",
    "    base_eval.rename(columns={\"anno_type4py\": \"anno\"}),\n",
    "    total=True\n",
    ")\n",
    "typet5_base_accuracy = experiments.predictions.performance(\n",
    "    base_eval.rename(columns={\"anno_typet5\": \"anno\"}),\n",
    "    total=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd919831-b72d-497d-9157-a6508094d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [\"observations\"]\n",
    "model_specific = [\"predictions\", \"stracc\", \"relacc\"]\n",
    "ground_truth_columns = typilus_adjusted_accuracy[common] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80be2c-2fd0-4983-9fc0-82c7d1728e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adjusted_baseline := pd.concat(\n",
    "    [ground_truth_columns, typilus_adjusted_accuracy[model_specific], type4py_adjusted_accuracy[model_specific], typet5_adjusted_accuracy[model_specific]],\n",
    "    keys=[\"groundtruth\", \"typilus\", \"type4py\", \"typet5\"],\n",
    "    axis=1\n",
    "))\n",
    "#|l|c|cc|cc|cc|\n",
    "print(adjusted_baseline.to_latex(\n",
    "    column_format=\"|l|c|cc|cc|cc|\",\n",
    "    float_format=\"%.2f\",\n",
    "    label=\"tbl:adjusted_ml_perf\",\n",
    "    caption=(\"Performance of ML models upon adjusted canonical form\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673eaaa-9aa6-46ae-92b4-1c7d2f2b449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(base_baseline := pd.concat(\n",
    "    [ground_truth_columns, typilus_base_accuracy[model_specific], type4py_base_accuracy[model_specific], typet5_base_accuracy[model_specific]],\n",
    "    keys=[\"groundtruth\", \"typilus\", \"type4py\", \"typet5\"],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "print(adjusted_baseline.to_latex(\n",
    "    label=\"tbl:base_ml_perf\",\n",
    "    caption=(\"Performance of ML models upon base canonical form\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d57c17-4f9c-4ba8-805a-d5f0056585a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(baseline_base := pd.concat(\n",
    "    [typilus_base_accuracy, type4py_base_accuracy, typet5_base_accuracy],\n",
    "    keys=[\"typilus\", \"type4py\", \"typet5\"],\n",
    "    axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d5326-a3f6-4a38-a8a3-c4dd3778a62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f13fdf6d-f84d-49de-bfba-3232e43b7842",
   "metadata": {},
   "source": [
    "# Baseline majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21d155-2cbd-499e-94f6-12c401214f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa5393-cab4-4c62-9fc8-f8ab00458b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.predictions\n",
    "import pandas as pd\n",
    "\n",
    "def hard_majority(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    predictions = df[prediction_annos]\n",
    "    \n",
    "    hm = predictions.mode(axis=\"columns\")[0]\n",
    "    # assert not hm.isna().any(), f\"NAs: {hm.isna().sum()}\"\n",
    "    return experiments.predictions.performance(pd.merge(\n",
    "        left=df, \n",
    "        right=hm.rename(\"anno\"), \n",
    "        left_index=True, \n",
    "        right_index=True\n",
    "    ), total=True)\n",
    "\n",
    "display(adjusted_hard_majority := hard_majority(adjusted_eval))\n",
    "display(base_hard_majority := hard_majority(base_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672634e-2a3b-47a4-ae97-737f86ad7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def soft_majority_vote(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def soft_majority(row) -> str:\n",
    "        weighted_votes = collections.defaultdict(float)\n",
    "        for pred_col, prob_col in zip(prediction_annos, probs):\n",
    "            # print(pred_col, type(row[pred_col]), prob_col, type(row[prob_col]))\n",
    "            weighted_votes[row[pred_col]] += row[prob_col]\n",
    "        \n",
    "        return max(weighted_votes, key=weighted_votes.get)\n",
    "\n",
    "    df[probs] = df[probs].fillna(0)\n",
    "    sm = df.apply(soft_majority, axis=1)\n",
    "    # assert not sm.isna().any(), f\"NAs: {sm.isna().sum()}\"\n",
    "    return experiments.predictions.performance(pd.merge(\n",
    "        left=df, \n",
    "        right=sm.rename(\"anno\"), \n",
    "        left_index=True, \n",
    "        right_index=True\n",
    "    ), total=True)\n",
    "\n",
    "display(adjusted_soft_majority := soft_majority_vote(adjusted_eval))\n",
    "display(base_soft_majority := soft_majority_vote(base_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6079f32-84e4-4bc3-a020-a789684e09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "identical_cols = [\"predictions\", \"observations\", \"unassigned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a48ec-7a32-4890-b08c-1fed27015dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_occ = adjusted_hard_majority[[\"predictions\", \"observations\"]]\n",
    "\n",
    "disp_adj_hm = adjusted_hard_majority.drop(columns=identical_cols)\n",
    "disp_adj_sm = adjusted_soft_majority.drop(columns=identical_cols)\n",
    "\n",
    "display(disp_adj_voting := pd.concat(\n",
    "    [pred_occ, disp_adj_hm, disp_adj_sm],\n",
    "    keys=[\"Statistics\", \"Hard Majority\", \"Soft Majority\"],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "print(disp_adj_voting.to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    #label=\"tbl:adjusted_ml_perf\",\n",
    "    caption=(\"Performance of Voting Procedures upon adjusted canonical form\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14875470-d375-46fd-8fdf-48161b48bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_occ = base_hard_majority[[\"predictions\", \"observations\"]]\n",
    "\n",
    "disp_base_hm = base_hard_majority.drop(columns=identical_cols)\n",
    "disp_base_sm = base_soft_majority.drop(columns=identical_cols)\n",
    "\n",
    "display(disp_base_voting := pd.concat(\n",
    "    [pred_occ, disp_base_hm, disp_base_sm],\n",
    "    keys=[\"Statistics\", \"Hard Majority\", \"Soft Majority\"],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "print(disp_base_voting.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a97be-9387-4394-9861-b084ef3ed43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(disp_voting := pd.concat(\n",
    "    [\n",
    "        pd.concat([pred_occ], keys=[\"Statistics\"], axis=1),\n",
    "        pd.concat([disp_adj_hm.drop(columns=\"matches\"), disp_base_hm.drop(columns=\"matches\")], keys=[\"adjusted\", \"base\"], axis=1),\n",
    "        pd.concat([disp_adj_sm.drop(columns=\"matches\"), disp_base_sm.drop(columns=\"matches\")], keys=[\"adjusted\", \"base\"], axis=1),\n",
    "    ],\n",
    "    keys=[\"\", \"Hard Majority\", \"Soft Majority\"], \n",
    "    axis=1\n",
    "))\n",
    "\n",
    "print(disp_voting.to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    label=\"tbl:voting_ml_perf\",\n",
    "    caption=(\"Performance of Voting Procedures upon adjusted \\& base canonical forms\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56acb1-de5f-4f1a-ab72-23935c08113d",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542e541-9cc8-4fef-931e-302b60589790",
   "metadata": {},
   "source": [
    "# Basheer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd637ab-3e5d-4aba-ae5c-e4c45fb1fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc, dataclasses, enum\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class BaheerMetadata:\n",
    "    repository: str\n",
    "    file: str\n",
    "    category: TypeCollectionCategory\n",
    "    qname_ssa: str\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class BaheerOpinion:\n",
    "    prediction: str\n",
    "    probability: float\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class BaheerResult:\n",
    "    strategy: str\n",
    "    agent_tag: str\n",
    "    metadata: BaheerMetadata\n",
    "    opinion: BaheerOpinion\n",
    "\n",
    "\n",
    "class Confidence(enum.IntEnum):\n",
    "    High = enum.auto()\n",
    "    Low = enum.auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def make(prob: float, threshold: float) -> \"Confidence\":\n",
    "        if prob >= threshold:\n",
    "            return Confidence.High\n",
    "        else:\n",
    "            return Confidence.Low\n",
    "\n",
    "class Strategy(abc.ABC):\n",
    "    def __init__(self, agent1_tag: str, agent2_tag: str) -> None:\n",
    "        self.agent1_tag = agent1_tag\n",
    "        self.agent2_tag = agent2_tag\n",
    "    \n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        ...\n",
    "\n",
    "\n",
    "class BaheerResolver(abc.ABC):\n",
    "    def __init__(self, agent1_tag: str, agent1_threshold: float, agent2_tag: str, agent2_threshold: float) -> None:\n",
    "        self.agent1_tag = agent1_tag\n",
    "        self.agent2_tag = agent2_tag\n",
    "\n",
    "        self.agent1_threshold = agent1_threshold\n",
    "        self.agent2_threshold = agent2_threshold\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def resolve(self, agent1_pred: str, agent1_prob: float, agent2_pred: str, agent2_prob: float) -> BaheerResult:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43e80a-e28b-44df-b18b-6aa3bd284e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submitting(Strategy):\n",
    "    \"\"\"Agent 1 submits to Agent 2\"\"\"\n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        return BaheerResult(strategy=\"submit\", agent_tag=self.agent2_tag, metadata=metadata, opinion=agent2)\n",
    "\n",
    "class Forcing(Strategy):\n",
    "    \"\"\"Agent 1 imposes its will over that of Agent 2\"\"\"\n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        return BaheerResult(strategy=\"force\", agent_tag=self.agent1_tag, metadata=metadata, opinion=agent1)\n",
    "\n",
    "class Delegation(Strategy):\n",
    "    def __init__(self, agent1_tag: str, agent2_tag: str, agent3_tag: str, agent3_opinions: dict[BaheerMetadata, BaheerOpinion]) -> None:\n",
    "        super().__init__(agent1_tag, agent2_tag)\n",
    "        self.agent3_tag = agent3_tag\n",
    "        self.agent3_opinions = agent3_opinions\n",
    "\n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        knowledge = self.agent3_opinions.get(metadata, (pd.NA, float(\"-inf\")))\n",
    "        return BaheerResult(strategy=\"delegate\", agent_tag=self.agent3_tag, *knowledge)\n",
    "    \n",
    "\n",
    "class Ignoring(Strategy):\n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        return BaheerResult(strategy=\"ignore\", agent_tag=\"IGNORED\", metadata=metadata, opinion=BaheerOpinion(prediction=pd.NA, probability=float(\"-inf\")))\n",
    "\n",
    "class Negotiation(Strategy):\n",
    "    def apply(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        if agent1.probability > agent2.probability:\n",
    "            return BaheerResult(strategy=\"negotiate\", agent_tag=self.agent1_tag, metadata=metadata, opinion=agent1)\n",
    "        else:\n",
    "            return BaheerResult(strategy=\"negotiate\", agent_tag=self.agent2_tag, metadata=metadata, opinion=agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f9e86-9467-417b-b54c-ee2ba81e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaheerStrongConflictResolver(BaheerResolver):\n",
    "    def __init__(self, agent1_tag: str, agent1_threshold: float, str, agent2_tag: str, agent2_threshold: float, agent3_opinions: dict[BaheerMetadata, BaheerOpinion]) -> None:\n",
    "        super().__init__(agent1_tag, agent1_threshold, agent2_tag, agent2_threshold)\n",
    "        self.agent3_opinions = agent3_opinions\n",
    "\n",
    "    def resolve(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion, agent3: pd.DataFrame) -> BaheerResult:\n",
    "        match Confidence.make(agent1.probability, self.agent1_threshold), Confidence.make(agent2.probability, self.agent2_threshold):\n",
    "            case (Confidence.High, Confidence.High) | (Confidence.Low, Confidence.Low):\n",
    "                return Delegation(self.agent1_tag, self.agent2_tag, self.agent3_opinions).apply(metadata, agent1, agent2)\n",
    "\n",
    "            case (Confidence.High, Confidence.Low):\n",
    "                return Forcing(self.agent1_tag, self.agent2_tag).apply(metadata, agent1, agent2)\n",
    "        \n",
    "            case (Confidence.Low, Confidence.High):\n",
    "                return Forcing(self.agent2_tag, self.agent1_tag).apply(metadata, agent2, agent1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6aa5-8d20-449f-a9f7-aaea145021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaheerWeakConflictResolver(BaheerResolver):\n",
    "    def resolve(self, metadata: BaheerMetadata, agent1: BaheerOpinion, agent2: BaheerOpinion) -> BaheerResult:\n",
    "        match Confidence.make(agent1.probability, self.agent1_threshold), Confidence.make(agent2.probability, self.agent2_threshold):\n",
    "            case (Confidence.Low, Confidence.Low):\n",
    "                return Ignoring(self.agent1_tag, self.agent2_tag).apply(metadata, agent1, agent2)\n",
    "\n",
    "            case (Confidence.High, Confidence.High):\n",
    "                return Negotiation(self.agent1_tag, self.agent2_tag).apply(metadata, agent1, agent2)\n",
    "\n",
    "            case (Confidence.High, Confidence.Low):\n",
    "                return Submitting(self.agent1_tag, self.agent2_tag).apply(metadata, agent1, agent2)\n",
    "\n",
    "            case (Confidence.Low, Confidence.High):\n",
    "                return Submitting(self.agent2_tag, self.agent1_tag).apply(metadata, agent2, agent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf0754-c6db-45f3-a43d-d4dc33e042d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baheer(\n",
    "    agent1: pd.DataFrame, agent1_tag: str, agent1_threshold: float,\n",
    "    agent2: pd.DataFrame, agent2_tag: str, agent2_threshold: float,\n",
    "    agent3: pd.DataFrame, agent3_tag: str\n",
    ") -> pd.DataFrame:\n",
    "    assert agent1.shape[0] == agent2.shape[0] == agent3.shape[0]\n",
    "    # conflict_level = (agent1.pred.fillna(\"<MISSING>\") == agent2.pred.fillna(\"<MISSING>\")).sum() / len(agent1.pred)\n",
    "    conflict_level = (agent1.pred == agent2.pred).sum() / len(agent1.pred)\n",
    "\n",
    "    print(conflict_level)\n",
    "\n",
    "    for agent in (agent1, agent2):\n",
    "        agent.pred = agent.pred.fillna(\"<MISSING>\")\n",
    "        agent.probability = agent.probability.fillna(0.0)\n",
    "            \n",
    "    if conflict_level >= 0.5:\n",
    "        negotiator: dict[BaheerMetadata, BaheerOpinion] = {\n",
    "            BaheerMetadata(repository, file, cat, qname_ssa): BaheerOpinion(pred, prob)\n",
    "            for file, cat, qname_ssa, pred, prob in agent3[[\"repository\", \"file\", \"category\", \"qname_ssa\", \"pred\", \"probability\"]].itertuples(index=False)\n",
    "            if pd.notna(pred)\n",
    "        }\n",
    "        resolver = BaheerStrongConflictResolver(agent1_tag, agent1_threshold, agent2_tag, agent2_threshold, agent3_tag, negotiator)\n",
    "    else:\n",
    "        resolver = BaheerWeakConflictResolver(agent1_tag, agent1_threshold, agent2_tag, agent2_threshold)\n",
    "\n",
    "    method = type(resolver).__qualname__\n",
    "\n",
    "    aligned = pd.merge(\n",
    "        left=agent1,\n",
    "        right=agent2,\n",
    "        on=[\"repository\", \"file\", \"category\", \"qname\", \"qname_ssa\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_agent1\", \"_agent2\")\n",
    "    )\n",
    "    \n",
    "    resolutions = []\n",
    "\n",
    "    for row in aligned[[\"repository\", \"file\", \"category\", \"qname\", \"qname_ssa\", \"pred_agent1\", \"probability_agent1\", \"pred_agent2\", \"probability_agent2\"]].itertuples(index=False):\n",
    "        # print(len(row), row)\n",
    "        repository, file, cat, qname, qname_ssa, agent1_pred, agent1_prob, agent2_pred, agent2_prob = row\n",
    "        metadata = BaheerMetadata(repository, file, cat, qname_ssa)\n",
    "        agent1_opinion = BaheerOpinion(agent1_pred, agent1_prob)\n",
    "        agent2_opinion = BaheerOpinion(agent2_pred, agent2_prob)\n",
    "    \n",
    "        resolution = resolver.resolve(metadata, agent1_opinion, agent2_opinion)\n",
    "        resolutions.append((\n",
    "            resolution.metadata.repository, resolution.metadata.file, resolution.metadata.category, \n",
    "            qname, qname_ssa,\n",
    "            resolution.agent_tag, resolution.opinion.prediction, resolution.opinion.probability, resolution.strategy\n",
    "        ))\n",
    "\n",
    "    df_resolutions = pd.DataFrame(resolutions, columns=[\"repository\", \"file\", \"category\", \"qname\", \"qname_ssa\", \"agent\", \"pred\", \"probability\", \"strategy\"]).assign(method=method)\n",
    "    return df_resolutions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9571911-d435-4962-ae28-6260f7435d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basheer implementation\n",
    "# assert BaheerStrongConflict(agent1_tag=\"father\", agent1_threshold=0.6, agent2_tag=\"mother\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a35465-287e-40bf-bbbc-56fe1df8bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, pandas as pd\n",
    "\n",
    "def baheer_frontend(model_predictions: pd.DataFrame, agent1_tag: str, agent1_threshold: float, agent2_tag: str, agent2_threshold: float, agent3_tag: str) -> None:\n",
    "    agent1_mapping = {\n",
    "        f\"anno_{agent1_tag}\": \"pred\",\n",
    "        f\"probability_{agent1_tag}\": \"probability\",\n",
    "    }\n",
    "    agent2_mapping = {\n",
    "        f\"anno_{agent2_tag}\": \"pred\",\n",
    "        f\"probability_{agent2_tag}\": \"probability\",\n",
    "    }\n",
    "    agent3_mapping = {\n",
    "        f\"anno_{agent3_tag}\": \"pred\",\n",
    "        f\"probability_{agent3_tag}\": \"probability\",\n",
    "    }\n",
    "\n",
    "    # print(adjusted_eval.columns)\n",
    "    \n",
    "    agent1 = model_predictions.drop(columns=[\"gt_anno\"]).rename(columns=agent1_mapping).drop(columns=list(agent2_mapping) + list(agent3_mapping))\n",
    "    agent2 = model_predictions.drop(columns=[\"gt_anno\"]).rename(columns=agent2_mapping).drop(columns=list(agent1_mapping) + list(agent3_mapping))\n",
    "    agent3 = model_predictions.drop(columns=[\"gt_anno\"]).rename(columns=agent3_mapping).drop(columns=list(agent1_mapping) + list(agent2_mapping))\n",
    "\n",
    "    # print(agent1.columns, agent2.columns, agent3.columns)\n",
    "\n",
    "    b = baheer(\n",
    "        agent1, agent1_tag, agent1_threshold, \n",
    "        agent2, agent2_tag, agent2_threshold, \n",
    "        agent3, agent3_tag\n",
    "    )\n",
    "    assert len(b) == len(model_predictions)\n",
    "\n",
    "    return b\n",
    "\n",
    "    #experiments.predictions.performance(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0cfa0-45e9-4684-a87e-9b35fd4c9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder\n",
    "\n",
    "precision_recall_fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "for model in (\"type4py\", \"typilus\", \"typet5\"):\n",
    "    for perf_eval, ax in zip([adjusted_eval, base_eval], [ax1, ax2]):        \n",
    "        PrecisionRecallDisplay.from_predictions(\n",
    "            y_true=perf_eval[f\"anno_{model}\"] == perf_eval[\"gt_anno\"],\n",
    "            y_pred=perf_eval[f\"probability_{model}\"],\n",
    "            name=model,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "ax1.set_title(\"Adjusted\")\n",
    "ax2.set_title(\"Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec62201-9737-40a3-9354-cdf296d9d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "configs = []\n",
    "\n",
    "agents = [\"type4py\", \"typilus\", \"typet5\"]\n",
    "# agent_thresholds = dict(zip(agents, [\n",
    "def baheer_experiment():\n",
    "    for agent1_tag, agent2_tag, agent3_tag in itertools.permutations(agents, r=3):\n",
    "        for agent1_threshold, agent2_threshold in itertools.combinations_with_replacement(np.linspace(0, 1, 21).tolist(), r=2):\n",
    "            adjusted_basheer = baheer_frontend(adjusted_eval, agent1_tag, agent1_threshold, agent2_tag, agent2_threshold, agent3_tag).rename(columns={\"pred\": \"anno\"})\n",
    "            assert len(adjusted_basheer) == len(adjusted_eval)    \n",
    "        \n",
    "            #print(f\"{agent1_tag=}, {agent1_threshold=}, {agent2_tag=}, {agent2_threshold=}, {agent3_tag=}\")\n",
    "            perf = pd.merge(\n",
    "                left=adjusted_basheer, right=adjusted_eval, on=[\"repository\", \"file\", \"category\", \"qname\", \"qname_ssa\"]\n",
    "            )\n",
    "            # display(perf[[\"repository\", \"file\", \"qname_ssa\", \"gt_anno\", \"anno\", \"strategy\"] + [f\"anno_{agent}\" for agent in agents]])\n",
    "            #print(perf[\"strategy\"].value_counts(normalize=True))\n",
    "            #print(perf[\"agent\"].value_counts(normalize=True))\n",
    "    \n",
    "            p = experiments.predictions.performance(perf)\n",
    "            configs.append((agent1_tag, agent2_tag, agent3_tag, agent1_threshold, agent2_threshold, p))\n",
    "        \n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acee20a-c17c-4014-8096-a000718c4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # configs.append((agent1_tag, agent2_tag, agent3_tag), (agent1_threshold, agent2_threshold), p)\n",
    "    config_df = pd.DataFrame(configs, columns=[\"agent1\", \"agent2\", \"agent3\", \"agent1_threshold\", \"agent2_threshold\", \"performance\"])\n",
    "    display(config_df.sort_values(by=[\"performance\"], ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
