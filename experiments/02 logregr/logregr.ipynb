{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f04853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benji/Documents/Uni/heidelberg/05/masterarbeit/impls/scripts/experiments\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pwd\n",
    "%cd /home/benji/Documents/Uni/heidelberg/05/masterarbeit/impls/scripts/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b48f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 17:09:23.652042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 17:09:24.470462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from scripts.common.output import ContextIO, ExtendedDatasetIO, InferredIO\n",
    "from scripts.common.schemas import ContextSymbolSchema, ExtendedTypeCollectionSchema, InferredSchema\n",
    "from scripts.common.schemas import TypeCollectionCategory, ContextCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbc71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "from pandera import typing as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39904488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from scripts.infer.structure import DatasetFolderStructure\n",
    "\n",
    "dataset = DatasetFolderStructure(pathlib.Path(\n",
    "    \"/home/benji/Documents/Uni/heidelberg/05/masterarbeit/datasets/better-types-4-py-dataset\"\n",
    "))\n",
    "ARTIFACT_ROOT = pathlib.Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eba8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inference_artifacts(tool: str, task: TypeCollectionCategory | str) -> pd.DataFrame:\n",
    "    return pd.concat([\n",
    "        InferredIO(artifact_root=ARTIFACT_ROOT, dataset=dataset, repository=project, tool_name=tool, task=task).read()\n",
    "        .assign(repository=project.name)\n",
    "        .drop_duplicates(subset=[\"repository\", ContextSymbolSchema.file, ContextSymbolSchema.category, ContextSymbolSchema.qname_ssa], keep=False)\n",
    "        for project in tqdm.tqdm(dataset.test_set(), desc=f\"Loading inference artifacts for {tool} @ {task}\")\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0a5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading context vectors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 90.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  file            category                                      qname  loop  reassigned  nested  flow_control  builtin_source  local_source  import_source                                  qname_ssa                repository  callable_return  callable_parameter  single_target_assign  ann_assign  aug_assign  multi_target_assign  instance_attribute  for_target  with_target\n",
      "0   tests/test_item.py            VARIABLE                             need_cssselect     0           0       0             0               0             0              0                           need_cssselectλ1  linw1995__data_extractor                0                   0                     1           0           0                    0                   0           0            0\n",
      "1   tests/test_item.py            VARIABLE                                  need_lxml     0           0       0             0               0             0              0                                need_lxmlλ1  linw1995__data_extractor                0                   0                     1           0           0                    0                   0           0            0\n",
      "2   tests/test_item.py     CALLABLE_RETURN                                   element0     0           0       0             0               0             0              0                                   element0  linw1995__data_extractor                1                   0                     0           0           0                    0                   0           0            0\n",
      "3   tests/test_item.py            VARIABLE                              element0.text     0           0       0             0               0             0              0                            element0.textλ1  linw1995__data_extractor                0                   0                     1           0           0                    0                   0           0            0\n",
      "4   tests/test_item.py     CALLABLE_RETURN                              item_property     0           0       0             0               0             0              0                              item_property  linw1995__data_extractor                1                   0                     0           0           0                    0                   0           0            0\n",
      "5   tests/test_item.py  CALLABLE_PARAMETER                      item_property.request     0           0       0             0               0             0              0                      item_property.request  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "6   tests/test_item.py     CALLABLE_RETURN                         test_field_extract     0           0       0             0               0             0              0                         test_field_extract  linw1995__data_extractor                1                   0                     0           0           0                    0                   0           0            0\n",
      "7   tests/test_item.py  CALLABLE_PARAMETER                test_field_extract.element0     0           0       0             0               0             0              0                test_field_extract.element0  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "8   tests/test_item.py  CALLABLE_PARAMETER               test_field_extract.Extractor     0           0       0             0               0             0              0               test_field_extract.Extractor  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "9   tests/test_item.py  CALLABLE_PARAMETER                    test_field_extract.expr     0           0       0             0               0             0              0                    test_field_extract.expr  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "10  tests/test_item.py  CALLABLE_PARAMETER                  test_field_extract.expect     0           0       0             0               0             0              0                  test_field_extract.expect  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "11  tests/test_item.py            VARIABLE                   test_field_extract.field     0           0       0             0               0             0              0                 test_field_extract.fieldλ1  linw1995__data_extractor                0                   0                     1           0           0                    0                   0           0            0\n",
      "12  tests/test_item.py     CALLABLE_RETURN            test_field_extract_with_is_many     0           0       0             0               0             0              0            test_field_extract_with_is_many  linw1995__data_extractor                1                   0                     0           0           0                    0                   0           0            0\n",
      "13  tests/test_item.py  CALLABLE_PARAMETER   test_field_extract_with_is_many.element0     0           0       0             0               0             0              0   test_field_extract_with_is_many.element0  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "14  tests/test_item.py  CALLABLE_PARAMETER  test_field_extract_with_is_many.Extractor     0           0       0             0               0             0              0  test_field_extract_with_is_many.Extractor  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "15  tests/test_item.py  CALLABLE_PARAMETER       test_field_extract_with_is_many.expr     0           0       0             0               0             0              0       test_field_extract_with_is_many.expr  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "16  tests/test_item.py  CALLABLE_PARAMETER     test_field_extract_with_is_many.expect     0           0       0             0               0             0              0     test_field_extract_with_is_many.expect  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n",
      "17  tests/test_item.py            VARIABLE      test_field_extract_with_is_many.field     0           0       0             0               0             0              0    test_field_extract_with_is_many.fieldλ1  linw1995__data_extractor                0                   0                     1           0           0                    0                   0           0            0\n",
      "18  tests/test_item.py     CALLABLE_RETURN            test_field_extract_with_default     0           0       0             0               0             0              0            test_field_extract_with_default  linw1995__data_extractor                1                   0                     0           0           0                    0                   0           0            0\n",
      "19  tests/test_item.py  CALLABLE_PARAMETER   test_field_extract_with_default.element0     0           0       0             0               0             0              0   test_field_extract_with_default.element0  linw1995__data_extractor                0                   1                     0           0           0                    0                   0           0            0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context_artifacts = pd.concat([\n",
    "    ContextIO(artifact_root=ARTIFACT_ROOT, dataset=dataset, repository=project).read()\n",
    "        .assign(repository=project.name)\n",
    "        .drop_duplicates(subset=[\"repository\", ContextSymbolSchema.file, ContextSymbolSchema.category, ContextSymbolSchema.qname_ssa], keep=False)\n",
    "    for project in tqdm.tqdm(dataset.test_set(), desc=\"Loading context vectors\")\n",
    "], ignore_index=True)\n",
    "\n",
    "dummified_mapping = {\n",
    "    f\"context_category_{category.value}\": category.name.lower()\n",
    "    for category in ContextCategory\n",
    "}\n",
    "\n",
    "context_artifacts = pd.get_dummies(context_artifacts, columns=[ContextSymbolSchema.context_category]).rename(columns=dummified_mapping)\n",
    "print(context_artifacts.head(n=20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213b9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ground truths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 73.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extended_ground_truths = pd.concat([\n",
    "    ExtendedDatasetIO(artifact_root=ARTIFACT_ROOT, dataset=dataset, repository=project).read()\n",
    "        .assign(repository=project.name)\n",
    "        .drop_duplicates(subset=[\"repository\", ContextSymbolSchema.file, ContextSymbolSchema.category, ContextSymbolSchema.qname_ssa], keep=False)\n",
    "    for project in tqdm.tqdm(dataset.test_set(), desc=\"Loading ground truths\")\n",
    "], ignore_index=True).fillna(pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6c900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_with_context = pd.merge(\n",
    "    left=extended_ground_truths,\n",
    "    right=context_artifacts,\n",
    "    on=[\"repository\", ContextSymbolSchema.file, ContextSymbolSchema.category, ContextSymbolSchema.qname_ssa],\n",
    "    how=\"inner\",\n",
    "    validate=\"1:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5aa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from scripts.dataset import normalisation\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RegressionArtifacts:\n",
    "    predictions_made: pd.DataFrame\n",
    "    matching_raw: pd.DataFrame\n",
    "    matching_depth_limited: pd.DataFrame\n",
    "    matching_adjusted: pd.DataFrame\n",
    "    matching_base: pd.DataFrame\n",
    "\n",
    "def create_inputs_for_regression(tool: str, task: TypeCollectionCategory | str) -> RegressionArtifacts:\n",
    "    tqdm.tqdm.pandas()\n",
    "    inference = load_inference_artifacts(tool=tool, task=task)\n",
    "    inference_vs_ground_truth_with_context = pd.merge(\n",
    "        left=inference,\n",
    "        right=ground_truths_with_context,\n",
    "        on=[\"repository\", ContextSymbolSchema.file, ContextSymbolSchema.category, ContextSymbolSchema.qname_ssa],\n",
    "        how=\"inner\",     # will drop keys for any tasks except 'all', which is exactly as intended\n",
    "        validate=\"1:1\"\n",
    "    )\n",
    "    \n",
    "    ### All samples\n",
    "    # Find where predictions were made\n",
    "    print(f\"=== {tool}/{task}: Scoring whether predictions were made ... === \")\n",
    "    predictions_made = inference_vs_ground_truth_with_context[InferredSchema.anno].notna()\n",
    "    predictions_made_df = inference_vs_ground_truth_with_context.assign(yscore=predictions_made).drop(columns=[\n",
    "        ExtendedTypeCollectionSchema.raw_anno, ExtendedTypeCollectionSchema.depth_limited_anno, \n",
    "        ExtendedTypeCollectionSchema.adjusted_anno, ExtendedTypeCollectionSchema.base_anno,\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    ### Only where predictions were made and we have a ground truth label\n",
    "    inference_vs_ground_truth_with_context = inference_vs_ground_truth_with_context[\n",
    "        inference_vs_ground_truth_with_context[InferredSchema.anno].notna() &\n",
    "        inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.raw_anno].notna()\n",
    "    ]\n",
    "    \n",
    "    # Find where lightly processed annotations line up\n",
    "    print(f\"=== {tool}/{task}: Scoring where predictions match ground truths exactly ... ===\")\n",
    "    matching_raw = inference_vs_ground_truth_with_context[InferredSchema.anno] == inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.raw_anno]\n",
    "    matching_raw_df = inference_vs_ground_truth_with_context.assign(yscore=matching_raw).drop(columns=[\n",
    "        ExtendedTypeCollectionSchema.depth_limited_anno, ExtendedTypeCollectionSchema.adjusted_anno, ExtendedTypeCollectionSchema.base_anno,\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Apply depth limiting, as every work except TypeT5 has done\n",
    "    print(f\"=== {tool}/{task}: Scoring where depth limited predictions match depth limited ground truths ... ===\")\n",
    "    depth_limited_inferred = inference_vs_ground_truth_with_context[InferredSchema.anno].progress_apply(normalisation.to_limited)\n",
    "    matching_depth_limited = depth_limited_inferred == inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.depth_limited_anno]\n",
    "    matching_depth_df = inference_vs_ground_truth_with_context.assign(inferred_depth_limited=depth_limited_inferred, yscore=matching_depth_limited).drop(columns=[\n",
    "        ExtendedTypeCollectionSchema.raw_anno, ExtendedTypeCollectionSchema.adjusted_anno, ExtendedTypeCollectionSchema.base_anno,\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Convert to adjusted annotations from depth limited annotations, without ground truth having typing.Any and None\n",
    "    print(f\"=== {tool}/{task}: Scoring where adjusted predictions match adjusted ground truths ... ===\")\n",
    "    print(f\"=== {tool}/{task}: Dropping Ground Truths with typing.Any, None ... ===\")\n",
    "    inference_vs_ground_truth_with_context = inference_vs_ground_truth_with_context[\n",
    "         ~inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.raw_anno].isin([\"typing.Any\", \"Any\", \"None\"])\n",
    "    ]\n",
    "    \n",
    "    # Recalculate depth limited for shaping reasons\n",
    "    depth_limited_inferred = inference_vs_ground_truth_with_context[InferredSchema.anno].progress_apply(normalisation.to_limited)\n",
    "    adjusted_anno = depth_limited_inferred.progress_apply(normalisation.to_adjusted)\n",
    "    matching_adjusted = adjusted_anno == inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.adjusted_anno]\n",
    "    matching_adjusted_df = inference_vs_ground_truth_with_context.assign(inferred_adjusted=adjusted_anno, yscore=matching_adjusted).drop(columns=[\n",
    "        ExtendedTypeCollectionSchema.raw_anno, ExtendedTypeCollectionSchema.depth_limited_anno, ExtendedTypeCollectionSchema.base_anno,\n",
    "    ])\n",
    "    \n",
    "    # Convert to base annotation to depth limited annotations, exclude none and any, as done in TypeT5\n",
    "    print(f\"{tool}/{task}: Scoring where base predictions match base ground truths ...\")\n",
    "    base_anno = depth_limited_inferred.progress_apply(normalisation.to_base)\n",
    "    matching_base = base_anno == inference_vs_ground_truth_with_context[ExtendedTypeCollectionSchema.base_anno]\n",
    "    matching_base_df = inference_vs_ground_truth_with_context.assign(inferred_base=base_anno, yscore=matching_base).drop(columns=[\n",
    "        ExtendedTypeCollectionSchema.raw_anno, ExtendedTypeCollectionSchema.depth_limited_anno, ExtendedTypeCollectionSchema.adjusted_anno,\n",
    "    ])\n",
    "    \n",
    "    print(\n",
    "        f\"{predictions_made_df.shape=}\", \n",
    "        f\"{matching_raw_df.shape=}\", \n",
    "        f\"{matching_depth_df.shape=}\", \n",
    "        f\"{matching_adjusted_df.shape=}\", \n",
    "        f\"{matching_base_df.shape=}\", \n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "    \n",
    "    return RegressionArtifacts(\n",
    "        predictions_made=predictions_made_df,\n",
    "        matching_raw=matching_raw_df,\n",
    "        matching_depth_limited=matching_depth_df,\n",
    "        matching_adjusted=matching_adjusted_df,\n",
    "        matching_base=matching_base_df\n",
    "    )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf644e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SET = list(dummified_mapping.values()) + [\n",
    "    ContextSymbolSchema.loop, ContextSymbolSchema.reassigned, ContextSymbolSchema.nested, ContextSymbolSchema.flow_control, ContextSymbolSchema.import_source, ContextSymbolSchema.builtin_source, ContextSymbolSchema.local_source, \n",
    "]\n",
    "\n",
    "FEATURE_SET_WITH_SCORE = FEATURE_SET + [\"yscore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "064c3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from sklearn import linear_model\n",
    "\n",
    "def regression(features, X, y) -> None:\n",
    "    lr = linear_model.LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"newton-cholesky\",\n",
    "    )\n",
    "    lr.fit(X, y)\n",
    "    pprint.pprint(dict(zip(features, lr.coef_[0])))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06f9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_made_regression(artifacts: RegressionArtifacts) -> None:\n",
    "    debug_feature_set = FEATURE_SET_WITH_SCORE + [InferredSchema.file, InferredSchema.qname, InferredSchema.anno]\n",
    "    \n",
    "    df = artifacts.predictions_made[debug_feature_set]\n",
    "    # print(df.head(n=20).to_string(), df.dtypes, sep=\"\\n\")\n",
    "    \n",
    "    df = artifacts.predictions_made[FEATURE_SET_WITH_SCORE].astype(int)\n",
    "    X, y = df[FEATURE_SET].to_numpy(), df[\"yscore\"]\n",
    "    \n",
    "    regression(FEATURE_SET, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "400d3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_raw_regression(artifacts: RegressionArtifacts) -> None:\n",
    "    debug_feature_set = FEATURE_SET_WITH_SCORE + [InferredSchema.file, ExtendedTypeCollectionSchema.raw_anno, InferredSchema.anno]\n",
    "\n",
    "    \n",
    "    df = artifacts.matching_raw[debug_feature_set]\n",
    "    # print(df.head(n=20).to_string(), df.dtypes, sep=\"\\n\")\n",
    "    \n",
    "    df = artifacts.matching_raw[FEATURE_SET_WITH_SCORE].astype(int)\n",
    "    X, y = df[FEATURE_SET].to_numpy(), df[\"yscore\"]\n",
    "    \n",
    "    regression(FEATURE_SET, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6947f6",
   "metadata": {},
   "source": [
    "# TypeT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af628c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading inference artifacts for TypeT5TopN1 @ CALLABLE_RETURN: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 133.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TypeT5TopN1/CALLABLE_RETURN: Scoring whether predictions were made ... === \n",
      "=== TypeT5TopN1/CALLABLE_RETURN: Scoring where predictions match ground truths exactly ... ===\n",
      "=== TypeT5TopN1/CALLABLE_RETURN: Scoring where depth limited predictions match depth limited ground truths ... ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4882/4882 [00:00<00:00, 59527.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TypeT5TopN1/CALLABLE_RETURN: Scoring where adjusted predictions match adjusted ground truths ... ===\n",
      "=== TypeT5TopN1/CALLABLE_RETURN: Dropping Ground Truths with typing.Any, None ... ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2800/2800 [00:00<00:00, 45608.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2800/2800 [00:00<00:00, 49543.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeT5TopN1/CALLABLE_RETURN: Scoring where base predictions match base ground truths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2800/2800 [00:00<00:00, 49109.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_made_df.shape=(10987, 27)\n",
      "matching_raw_df.shape=(4882, 28)\n",
      "matching_depth_df.shape=(4882, 29)\n",
      "matching_adjusted_df.shape=(2800, 29)\n",
      "matching_base_df.shape=(2800, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "type_t5_return = create_inputs_for_regression(tool=\"TypeT5TopN1\", task=TypeCollectionCategory.CALLABLE_RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e562e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.98472350e-13  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -6.39641387e-02 -3.60268302e+00\n",
      "  -1.21478225e+00 -6.95137446e-01 -1.42034682e-01 -1.60439769e+00]]\n"
     ]
    }
   ],
   "source": [
    "prediction_made_regression(artifacts=type_t5_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44684c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.61065950e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.15497748e-01 -2.09162998e+00  6.60451657e-01 -2.47879189e-01]]\n"
     ]
    }
   ],
   "source": [
    "matching_raw_regression(artifacts=type_t5_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6008184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading inference artifacts for HiType4PyN1 @ all: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 89.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HiType4PyN1/all: Scoring whether predictions were made ... === \n",
      "=== HiType4PyN1/all: Scoring where predictions match ground truths exactly ... ===\n",
      "=== HiType4PyN1/all: Scoring where depth limited predictions match depth limited ground truths ... ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14453/14453 [00:00<00:00, 61906.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HiType4PyN1/all: Scoring where adjusted predictions match adjusted ground truths ... ===\n",
      "=== HiType4PyN1/all: Dropping Ground Truths with typing.Any, None ... ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12116/12116 [00:00<00:00, 55771.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12116/12116 [00:00<00:00, 61175.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiType4PyN1/all: Scoring where base predictions match base ground truths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12116/12116 [00:00<00:00, 55960.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_made_df.shape=(58480, 27)\n",
      "matching_raw_df.shape=(14453, 28)\n",
      "matching_depth_df.shape=(14453, 29)\n",
      "matching_adjusted_df.shape=(12116, 29)\n",
      "matching_base_df.shape=(12116, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HiType4PyN1_all = create_inputs_for_regression(tool=\"HiType4PyN1\", task=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a138db00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_assign': 1.48596739784677,\n",
      " 'aug_assign': -2.962645779752652,\n",
      " 'builtin_source': 1.5309296095228195,\n",
      " 'callable_parameter': 2.293107243243585,\n",
      " 'callable_return': 3.6541024962823005,\n",
      " 'flow_control': 0.4412666776085077,\n",
      " 'for_target': -3.5969047853848273,\n",
      " 'import_source': 1.0964056796797494,\n",
      " 'instance_attribute': 3.2283371121406845,\n",
      " 'local_source': 0.6820853419693751,\n",
      " 'loop': -0.21829452191168996,\n",
      " 'multi_target_assign': -3.591541185530923,\n",
      " 'nested': -3.8777453478979926,\n",
      " 'reassigned': 0.5322803036782469,\n",
      " 'single_target_assign': 2.507964232802009,\n",
      " 'with_target': -3.018386731659758}\n"
     ]
    }
   ],
   "source": [
    "prediction_made_regression(artifacts=HiType4PyN1_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a97daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_assign': 0.6405012464295174,\n",
      " 'aug_assign': 0.0,\n",
      " 'builtin_source': -0.6125494615718191,\n",
      " 'callable_parameter': 2.116033083121507,\n",
      " 'callable_return': 2.731149781703476,\n",
      " 'flow_control': 0.06942834060108434,\n",
      " 'for_target': 0.0,\n",
      " 'import_source': -3.190280107607923,\n",
      " 'instance_attribute': -5.487684111253896,\n",
      " 'local_source': -3.584661965126871,\n",
      " 'loop': -0.52557555736792,\n",
      " 'multi_target_assign': 0.0,\n",
      " 'nested': -0.08106220110543441,\n",
      " 'reassigned': -0.10474777577665098,\n",
      " 'single_target_assign': 0.0,\n",
      " 'with_target': 0.0}\n"
     ]
    }
   ],
   "source": [
    "matching_raw_regression(artifacts=HiType4PyN1_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
